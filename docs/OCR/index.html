<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-OCR/ocr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.1">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Tốc độ là quan trọng RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Tốc độ là quan trọng Atom Feed">



<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-31VZQCNSCZ"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-31VZQCNSCZ",{anonymize_ip:!0})</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Nhận dạng ký tự quang học (OCR) | Tốc độ là quan trọng</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://your-docusaurus-test-site.com/docs/OCR/"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Nhận dạng ký tự quang học (OCR) | Tốc độ là quan trọng"><meta data-rh="true" name="description" content="I. Giới thiệu"><meta data-rh="true" property="og:description" content="I. Giới thiệu"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-test-site.com/docs/OCR/"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/OCR/" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-test-site.com/docs/OCR/" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.16147dea.css">
<link rel="preload" href="/assets/js/runtime~main.9013f7a9.js" as="script">
<link rel="preload" href="/assets/js/main.1814ed7f.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/mle.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/mle.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">ML Scale</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Hướng dẫn</a><a class="navbar__item navbar__link" href="/blog">Tác giả</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bangoc123/scale-machine-learning-on-production" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/Lấy-mẫu-con/">Lấy mẫu con</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/docs/OCR/">Nhận dạng ký tự quang học (OCR)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Giới thiệu</a></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Nhận dạng ký tự quang học (OCR)</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Nhận dạng ký tự quang học (OCR)</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="i-giới-thiệu">I. Giới thiệu<a class="hash-link" href="#i-giới-thiệu" title="Direct link to heading">​</a></h2><p>Nhận dạng ký tự quang học (<em>Optical Character Recognition</em> – OCR) đề cập đến 1 tập hợp các vấn đề về thị giác máy tính với mục đích chuyển đổi các hình ảnh kỹ thuật số hoặc hình ảnh tài liệu được scan thành dưới dạng văn bản mà máy tính có thể xử lý, lưu trữ và chỉnh sửa được như 1 tập tin văn bản thông thường hoặc dưới dạng 1 phần của phần mềm nhập liệu.</p><p>Nói dông dài vậy thôi chứ 1 cách dễ hiểu thì <code>OCR</code> đơn giản chỉ là 1 bài toán chuyển đổi đầu vào là 1 ảnh có text, thành đầu ra là các text trong ảnh đó dưới dạng kỹ thuật số. Các hình ảnh có thể bao gồm tài liệu, hóa đơn, biểu mẫu pháp lý, chứng minh thư hoặc những thứ trong môi trường tự nhiên (<em>OCR in the wild</em>) như biển báo đường phố, biển số xe, ...</p><p><img loading="lazy" src="/assets/images/ocr-0f561411d36289cedfa4a7faeaf91c13.png" width="1706" height="477" class="img_ev3q"></p><p>Dù thường không được chú ý, nhưng <code>OCR</code> là 1 trợ giúp không thể thay thế khi ta nói về tự động hóa. Nó giúp loại bỏ các quy trình không cần thiết của các tài liệu giấy, cho phép ta phân loại, sắp xếp, lưu trữ, quản lý và chia sẻ thông tin, đồng thời tránh các rủi ro bảo mật liên quan đến bản chất vật lý của các tài liệu giấy.</p><p>Ngoài ra, tự động hóa dựa trên <code>OCR</code> không chỉ là chia sẻ thông tin dưới dạng kỹ thuật số. Khi có nhiều tài liệu, các loại máy móc có thể sử dụng chúng làm mục nhập dữ liệu để tìm kiếm các mẫu và xu hướng. Việc trực quan hóa cũng trở nên dễ dàng hơn: nếu ta cần 1 biểu đồ, lược đồ hoặc bảng tính, thì việc sử dụng các tài liệu kỹ thuật số sẽ nhanh hơn nhiều so với việc biên soạn 1 báo cáo trực quan bằng tay.</p><p><code>OCR</code> cho phép ta dành ít thời gian hơn để xử lý từng tài liệu mới, tiết kiệm chi phí nhân lực và thay vào đó tập trung vào các chiến lược gia tăng giá trị. Đây là 1 chủ đề phức tạp vì nó là sự dụng hòa giữa 2 lĩnh vực lớn trong AI:</p><ul><li>Thị giác máy tính (<em>Computer Vision</em> – CV): huấn luyện các mô hình học máy để nhìn và giải thích hình ảnh theo cách tương tự như cách con người thực hiện.</li><li>Xử lý ngôn ngữ tự nhiên (<em>Natural Language Processing</em> – NLP): chủ yếu xử lý văn bản hay các dữ liệu chuyển giọng nói thành văn bản và tập trung vào việc dạy cho máy tính hiểu lời nói của con người.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-các-khái-niệm-cơ-bản">1. Các khái niệm cơ bản<a class="hash-link" href="#1-các-khái-niệm-cơ-bản" title="Direct link to heading">​</a></h3><p>Thường có sự pha trộn qua lại giữa 3 khái niệm: nhận dạng ký tự quang (<em>Optical Character Recognition</em> – <code>OCR</code>), nhận dạng ký tự thông minh (<em>Intelligent Character Recognition</em> – <code>ICR</code>) và thu thập dữ liệu thông minh (<em>Intelligent Data Capture</em> – <code>IDC</code>). Một số nguồn có xu hướng sử dụng các thuật ngữ này như các từ đồng nghĩa, nhưng có một sự khác biệt khá rõ ràng giữa chúng.</p><p>Thuật ngữ <code>OCR</code> thông thường được sử dụng phổ biến nhất cho văn bản trong các tài liệu in tiêu chuẩn, có cấu trúc. <code>ICR</code> là một phiên bản tiên tiến hơn của <code>OCR</code> được sử dụng cho các văn bản viết tay (handwritten), do sự phát triển của các công nghệ <code>OCR</code> hiện đại, các nhà khoa học dữ liệu và các kỹ sư hiếm khi phân biệt 2 hình thức thu thập dữ liệu tự động này. <code>IDC</code> đại diện cho các thuật toán được xây dựng để tự động hóa tốt hơn <code>OCR</code> vào các quy trình kinh doanh, nó kết hợp khả năng nhận dạng của <code>OCR</code> với việc giải thích dữ liệu (<em>Data Interpretation</em>) cho phép phân loại tài liệu và cả điểm nhập liệu.</p><p>Hiện tại, <code>OCR</code> không chỉ giới hạn ở nhận dạng văn bản tài liệu hoặc sách mà còn bao gồm các hình ảnh chứa văn bản được chụp trong các môi trường hỗn tạp hay không đồng nhất, hình thành nên các vấn đề như background phức tạp, nhiễu, ánh sáng, font chữ khác nhau và biến dạng hình học trong ảnh, ...</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-phân-loại-hình-ảnh-chứa-văn-bản">2. Phân loại hình ảnh chứa văn bản<a class="hash-link" href="#2-phân-loại-hình-ảnh-chứa-văn-bản" title="Direct link to heading">​</a></h3><p>Các thách thức trong <code>OCR</code> phát sinh chủ yếu do các tác vụ <code>OCR</code> đang thực hiện. Trên mạng mình có thấy nhiều bài viết phân loại các hình ảnh chứa văn bản này thành 2 loại (Có và Phi cấu trúc) nhưng mình thấy chia như vậy vẫn còn khá nhập nhằng và nhiều chỗ chưa được rõ ràng nên mình sẽ chia các tác vụ này thành 3 loại:</p><ul><li><strong>Có cấu trúc</strong> (<em>Structured</em>): văn bản trong các tài liệu được đánh máy, có background sạch, đồng nhất, font chữ theo tiêu chuẩn, ít nhiễu (<em>Noise</em>), hàng lối rõ ràng, có một trật tự nhất định và thường có mật độ văn bản (<em>Density</em>) cao.
<a href="https://nanonets.com/blog/content/images/2019/08/image-16.png" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://nanonets.com/blog/content/images/2019/08/image-16.png" class="img_ev3q"></a></li><li><strong>Phi cấu trúc</strong> (<em>Unstructured</em>): đây là loại ảnh nhiều khó khăn nhất, văn bản sẽ ở những vị trí ngẫu nhiên trong ảnh hay trong một hoạt cảnh tự nhiên (<em>OCR in the wild</em>), thường có mật độ văn bản thưa thớt, cấu trúc hàng lối bất định, background phức tạp, và không có font chuẩn. Dữ liệu kiểu này cũng có thể được chia thành 3 loại nhỏ khác:<ul><li><strong>Graphic text</strong>: ảnh có văn bản được thêm vào sau khi đã có ảnh như phụ đề của video, …</li><li><strong>Scene text</strong>: ảnh có văn bản xuất hiện trong tự nhiên, tức văn bản là thành phần có sẵn trong ảnh. Các dữ liệu loại này có nhiều thách thức như về hướng (<em>Orientation</em>), loại font, điều kiện ánh sáng hay độ lệch (<em>Skewness</em>) của chữ. Tác vụ OCR cho dữ liệu loại này còn gọi là <strong>Scene Text Recognition</strong> – <code>STR</code>.
<a href="http://tutorials.aiclub.cs.uit.edu.vn/wp-content/uploads/2021/07/graphicvsscenetext.jpg" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="http://tutorials.aiclub.cs.uit.edu.vn/wp-content/uploads/2021/07/graphicvsscenetext.jpg" class="img_ev3q"></a></li><li><strong>Synthetic text</strong>: khá giống với <strong>Graphic text</strong>, đây là một ý tưởng khá hay để cải thiện hiệu suất của quá trình huấn luyện bằng cách tạo dữ liệu giả bằng máy tính. Việc sinh các ký tự hoặc từ ngẫu nhiên lên một hình ảnh sẽ có vẻ tự nhiên hơn nhiều so với bất kỳ vật thể nào khác vì tính chất phẳng của văn bản. Các bộ dataset cho dữ liệu loại này cũng vượt trội về khả năng tạo ra các ngôn ngữ khác nhau, ngay cả những ngôn ngữ khó, chẳng hạn như tiếng Trung, tiếng Do Thái, tiếng Ả Rập, …
<a href="https://github.com/ankush-me/SynthText/raw/master/samples.png" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://github.com/ankush-me/SynthText/raw/master/samples.png" class="img_ev3q"></a></li></ul></li><li><strong>Lai</strong> giữa có và phi cấu trúc (<em>Hybrid</em>): thường là các bản scan, photocopy không phải ở dạng đánh máy của các tài liệu, sách, có cấu trúc tốt, background ít phức tạp, gọn gàng, thường có mật độ văn bản cao hơn so với các ảnh chứa văn bản phi cấu trúc nhưng có hướng nghiêng và độ lệch nhỏ. Ngoài ra, đối với loại văn bản như các tư liệu lịch sử thì còn có thể có sự những thay đổi lớn và mơ hồ do sự khác nhau về nét chữ giữa từng người hay chất lượng ảnh có thể bị giảm dần theo thời gian.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-ocr-và-deep-learning">3. OCR và Deep Learning<a class="hash-link" href="#3-ocr-và-deep-learning" title="Direct link to heading">​</a></h3><p><code>OCR</code> là 1 trong những nhiệm vụ thị giác máy tính được giải quyết sớm nhất vì ở khía cạnh nào đó, nó không cần tới <em>Deep Learning</em>, do đó đã có những cách triển khai <code>OCR</code> khác nhau ngay cả trước khi <em>Deep Learning</em> bùng nổ vào năm 2012. Điều này khiến nhiều người nghĩ rằng các bài toán <code>OCR</code> đã được “giải”, nó không còn là 1 thách thức nữa và việc sử dụng Deep Learning cho <code>OCR</code> là 1 việc làm quá mức cần thiết.</p><p>Trên thực tế, <code>OCR</code> chỉ mang lại kết quả tốt trong các trường hợp cụ thể và đúng là có những giải pháp tốt cho 1 số tác vụ <code>OCR</code> không yêu cầu Deep Learning. Rất nhiều kỹ thuật trước đó đã giải quyết vấn đề <code>OCR</code> cho văn bản có cấu trúc bằng 1 số kỹ thuật thị giác máy tính truyền thống hay các phương pháp xử lý ảnh căn bản như bộ lọc hình ảnh, hình thái học (<em>morphology</em>) và phát hiện cạnh (<em>contour</em>) để từ đó phân loại 1 vùng ảnh là chữ gì, … Các kỹ thuật này chỉ hoạt động tốt trên các tập dữ liệu hẹp, theo khuôn mẫu (<em>template-based</em>), không thay đổi nhiều về hướng, vị trí văn bản hay chất lượng hình ảnh, … nhưng chúng không hoạt động đúng với các môi trường ảnh không bị ràng buộc khác, có mật độ văn bản nhỏ hơn hay có các thuộc tính khác với dữ liệu có cấu trúc, …</p><p>Nhìn chung, đây vẫn được xem là thách thức, để làm cho các mô hình trở nên mạnh mẽ với các biến thể trên, giúp các doanh nghiệp có thể triển khai các ứng dụng học máy của họ trên quy mô lớn, các giải pháp mới cần được đưa ra. Các phương pháp tiếp cận bằng <em>Deep Learning</em> đã được cải thiện trong vài năm qua, làm hồi sinh mối quan tâm đến vấn đề <code>OCR</code>, nơi mạng nơ-ron có thể kết hợp các tác vụ xác định văn bản trong 1 hình ảnh và hiểu văn bản đó là gì. <em>Deep Learning</em> có thể được xem là bắt buộc để tiến tới các giải pháp tốt hơn và tổng quát hơn.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-các-bước-triển-khai-chính">4. Các bước triển khai chính<a class="hash-link" href="#4-các-bước-triển-khai-chính" title="Direct link to heading">​</a></h3><p>Có 2 mức độ để triển khai các tác vụ <code>OCR</code>: mức chuỗi (<em>sequence level</em>) và mức ký tự (<em>character level</em>). Ngày nay, mức chuỗi thường được ưa chuộng hơn do mức ký tự đòi hỏi chi phí gán nhãn quá lớn vì cần phải vẽ <code>bounding box</code> cho từng ký tự, từ đó dễ xuất hiện nhiều vấn đề nan giải hơn như <code>bounding box</code> giữa 2 ký tự liên tiếp có thể không rõ ràng hoặc bị đè lên nhau (overlap), ... Tiếp theo, người ta cũng chia <code>OCR</code> thành 2 bài toán con chính:</p><ul><li><p><strong>Phát hiện văn bản</strong> (<em>Text Detection</em>): phát hiện vùng ảnh có chứa văn bản. Đầu vào là ảnh, đầu ra là các <code>bounding box</code> bao quanh các phần văn bản được tìm thấy trên ảnh.</p></li><li><p><strong>Nhận dạng văn bản</strong> (<em>Text Recognition</em>): sau khi detect được các <code>bounding box</code> hay các vùng ảnh có chứa văn bản, ta tách riêng từng vùng ảnh này ra từ ảnh gốc, tạo thành các phần ảnh nhỏ còn gọi là các <code>Patch</code>. Đầu vào lúc này sẽ là <code>Patch</code> và đầu ra là văn bản có trong <code>Patch</code> đó.</p><p><img loading="lazy" src="/assets/images/pipeline-d744b428bc5315a50e08bff803fb4e1a.png" width="856" height="693" class="img_ev3q"></p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-một-số-dataset-cho-văn-bản-phi-cấu-trúc">5. Một số dataset cho văn bản phi cấu trúc<a class="hash-link" href="#5-một-số-dataset-cho-văn-bản-phi-cấu-trúc" title="Direct link to heading">​</a></h3><p>Có rất nhiều bộ dữ liệu hình ảnh có sẵn cho các văn bản phi cấu trúc và cho tiếng Anh, nhưng sẽ khó hơn để tìm bộ dữ liệu giống vậy cho các ngôn ngữ khác hay không phải cho văn bản phi cấu trúc. Các bộ dữ liệu khác nhau trình bày các tác vụ khác nhau cần giải quyết. Dưới đây là một vài bộ dữ liệu nổi tiếng:</p><ul><li><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener noreferrer">SVHN</a>: bộ dữ liệu về số nhà được trích xuất từ Google Street View. Các chữ số có nhiều hình dạng và kiểu viết khác nhau; tuy nhiên, mỗi số nhà được đặt ở giữa hình. Ảnh có độ phân giải không cao và cách sắp xếp của chúng có thể hơi kỳ lạ.</li><li>ICDAR <a href="https://paperswithcode.com/dataset/icdar-2013" target="_blank" rel="noopener noreferrer">2013</a> và <a href="https://paperswithcode.com/dataset/icdar-2015" target="_blank" rel="noopener noreferrer">2015</a>: 2 bộ dữ liệu dành cho hội nghị và cuộc thi ICDAR. Đây là 2 bộ dữ liệu benchmark tiêu chuẩn dùng để đánh giá rất nhiểu mô hình phục vụ các tác vụ OCR cho ảnh có văn bản gần nằm ngang. Ngoài ra còn có các phiên bản cho những năm khác như <a href="https://paperswithcode.com/dataset/icdar-2003" target="_blank" rel="noopener noreferrer">2003</a> hay <a href="https://rrc.cvc.uab.es/?ch=15" target="_blank" rel="noopener noreferrer">2019</a>, tuy nhiên các bản năm <a href="https://paperswithcode.com/dataset/icdar-2013" target="_blank" rel="noopener noreferrer">2013</a> và <a href="https://paperswithcode.com/dataset/icdar-2015" target="_blank" rel="noopener noreferrer">2015</a> vẫn là phổ biến nhất.</li><li><a href="https://paperswithcode.com/dataset/total-text" target="_blank" rel="noopener noreferrer">Total-Text</a>: tương tự các bộ dữ liệu ICDAR, đây cũng là một trong các bộ dữ liệu benchmark tiêu chuẩn, gồm các hình ảnh với nhiều loại văn bản khác nhau như các trường hợp văn bản ngang, nhiều hướng hay bị cong.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-một-số-công-cụ-mã-nguồn-mở">6. Một số công cụ mã nguồn mở<a class="hash-link" href="#6-một-số-công-cụ-mã-nguồn-mở" title="Direct link to heading">​</a></h3><p>Trong một thời gian dài, <a href="https://github.com/tesseract-ocr/tesseract" target="_blank" rel="noopener noreferrer">Tesseract OCR</a> dẫn đầu các tool OCR mã nguồn mở, có thể nhận dạng hơn 100 ngôn ngữ. <a href="https://github.com/tesseract-ocr/tesseract" target="_blank" rel="noopener noreferrer">Tesseract 4</a> sử dụng các mô hình LSTM cho <strong>Text Recognition</strong> thay vì các phương pháp thị giác máy truyền thống như các phiên bản trước nhưng vẫn tỏ ra khá yếu với các loại dữ liệu phi cấu trúc và 1 nhược điểm khác là <a href="https://github.com/tesseract-ocr/tesseract" target="_blank" rel="noopener noreferrer">Tesseract</a> chỉ được tối ưu cho CPU. Nhờ sự phát triển của <em>Deep Learning</em>, giờ đây ta đã có nhiều lựa chọn vượt trội hơn <a href="https://github.com/tesseract-ocr/tesseract" target="_blank" rel="noopener noreferrer">Tesseract</a>, sử dụng được cho nhiều ngôn ngữ hay các trường hợp dữ liệu khác nhau, hỗ trợ cả việc huấn luyện lại (<em>Retraining</em>) hoặc tinh chỉnh (<em>Fine-tuning</em>). Chúng liên tục được phát triển và sử dụng các phương pháp tiếp cận hiện đại nhất cho cả 2 bài toán <strong>Detection</strong> và <strong>Recognition</strong>. Một số đại diện nổi bật có thể kể đến là <a href="https://github.com/mindee/doctr" target="_blank" rel="noopener noreferrer">docTR</a>, <a href="https://github.com/faustomorales/keras-ocr" target="_blank" rel="noopener noreferrer">keras-ocr</a> hay <a href="https://github.com/JaidedAI/EasyOCR" target="_blank" rel="noopener noreferrer">EasyOCR</a> sử dụng pipeline được đề cập trong phần <a href="#e-b%E1%BB%95-sung">II.3.e</a>.</p><p>Ngoài ra, còn có các công cụ hỗ trợ việc sinh <strong>Synthetic text</strong> như <a href="https://github.com/ankush-me/SynthText" target="_blank" rel="noopener noreferrer">SynthText</a> hỗ trợ việc “rắc” chữ một cách thông minh để chúng trông chân thật nhất hay <a href="https://github.com/Belval/TextRecognitionDataGenerator" target="_blank" rel="noopener noreferrer">TRDG</a> hỗ trợ tạo dữ liệu <strong>Synthetic text</strong> cho <strong>Text Recognition</strong>, đây cũng là trình tạo dữ liệu được dùng trong <a href="https://github.com/JaidedAI/EasyOCR" target="_blank" rel="noopener noreferrer">EasyOCR</a>.</p><p>Cuối cùng, hội tụ của tất cả các công nghệ trên, <a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" rel="noopener noreferrer">PaddleOCR</a> – một bộ công cụ hay nói đúng hơn là một hệ sinh thái cho OCR cực kỳ mạnh mẽ nhưng lại cực kỳ nhẹ (ultra lightweight), hỗ trợ đa ngôn ngữ cũng như nhiều thuật toán tiên tiến liên quan đến OCR và phát triển các mô hình hay giải pháp công nghiệp nổi bật. Đồng thời, <a href="https://github.com/PaddlePaddle/PaddleOCR" target="_blank" rel="noopener noreferrer">PaddleOCR</a> cũng cung cấp các công cụ gán nhãn và tạo dữ liệu Synthetic, ngoài ra bộ công cụ này còn giúp huấn luyện và triển khai giữa các thiết bị máy chủ, di động hay các thiết bị nhúng và IoT.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ii-các-phương-pháp-tiếp-cận">II. Các phương pháp tiếp cận<a class="hash-link" href="#ii-các-phương-pháp-tiếp-cận" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-khởi-nguồn-và-lý-do-tiếp-cận-bằng-deep-learning">1. Khởi nguồn và lý do tiếp cận bằng Deep Learning<a class="hash-link" href="#1-khởi-nguồn-và-lý-do-tiếp-cận-bằng-deep-learning" title="Direct link to heading">​</a></h3><p>Như đã đề cập trong <a href="#3-ocr-v%C3%A0-deep-learning">I.3</a>, một trong những hướng tiếp cận đơn giản nhất cho bài toán OCR là sử dụng các kỹ thuật thị giác máy tính truyền thống hay các phương pháp xử lý ảnh căn bản và chúng đã được sử dụng trong một thời gian dài. Có thể gói gọn chúng trong các bước sau:</p><ul><li>Áp dụng các bộ lọc (<em>filters</em>) để làm nổi bật các ký tự từ background.</li><li>Tiến hành phát hiện cạnh (<em>contour</em>) để phát hiện các vùng ảnh chứa ký tự.</li><li>Thực hiện phân loại cho các vùng ảnh đó để xác định ký tự trong chúng là gì.</li></ul><p>Rõ ràng là nếu bước 2 được thực hiện tốt thì bước 3 có thể dễ dàng thực hiện bằng cách khớp mẫu (<em>Pattern Matching</em>) hay huấn luyện 1 mạng <code>CNN</code> đơn giản. Tuy nhiên, phát hiện cạnh sẽ khá khó khăn cho việc khái quát hóa, chưa thể thích ứng được với các sự đa dạng trong ảnh, nhất là trong các môi trường hỗn tạp. Nó đòi hỏi rất nhiều điều chỉnh thủ công nên cách làm này trở nên không khả thi trong hầu hết các trường hợp. Ví dụ như hình bên dưới, xử lý ảnh căn bản thất bại khi các ký tự gần nhau.</p><p><img loading="lazy" src="/assets/images/basic-failed-ec5515633d10def4d51ffd055d069906.png" width="603" height="243" class="img_ev3q"></p><p>Một nhược điểm nữa cũng có thể thấy là đa phần cách làm này chỉ có hiệu quả với mức ký tự nên nếu triển khai theo hướng này thì ngoài chi phí gán nhãn lớn khi làm dữ liệu, ta còn có thể sẽ đánh mất ngữ nghĩa xung quanh text. Các hướng tiếp cận bằng <em>Deep Learning</em> hiện nay có thể xem là đã chín mùi, vượt trội trong việc khái quát của chúng và đã trở thành giải pháp thống trị trong cả nghiên cứu và thực tế.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-phát-hiện-văn-bản-text-detection">2. Phát hiện văn bản (Text Detection)<a class="hash-link" href="#2-phát-hiện-văn-bản-text-detection" title="Direct link to heading">​</a></h3><p>Đầu tiên, có thể thấy bài toán <strong>Text Detection</strong> khá giống với bài toán <strong>Object Detection</strong>, trong đó object cần được phát hiện chính là text. Do vậy, ta có thể áp dụng các kiến trúc <em>Deep Learning</em> cho bài toán này như YOLO, SSD hay Faster R-CNN để đạt được độ chính xác cao hơn so với các phương pháp xử lý ảnh thủ công. Tuy nhiên, các mô hình này có vẻ chỉ mang lại kết quả tốt với các object lớn cũng như với ảnh có độ phân giải cao. Khá là trớ trêu vì trên thực tế, các mô hình này tỏ ra khó khăn hơn và có xu hướng không đạt được độ chính xác mong muốn khi detect các chữ số và chữ cái so với khi detect các object phức tạp như chó, mèo hay con người.</p><p>Các phương pháp chuyên biệt dựa trên <em>Deep Learning</em> gần đây đã giải quyết được hầu hết các vấn đề trên và đạt được những kết quả tốt trên nhiều bộ dữ liệu benchmark tiêu chuẩn như ICDAR <a href="https://paperswithcode.com/dataset/icdar-2013" target="_blank" rel="noopener noreferrer">2013</a> và <a href="https://paperswithcode.com/dataset/icdar-2015" target="_blank" rel="noopener noreferrer">2015</a> hay <a href="https://paperswithcode.com/dataset/total-text" target="_blank" rel="noopener noreferrer">Total-Text</a>. Cụ thể, chúng được chia thành 2 loại chính như sau:</p><ul><li>Các phương pháp <strong>Regression-based</strong>: sau khi có kết quả dự đoán, <code>bounding box</code> cuối cùng sẽ được lọc qua bằng thuật toán <strong>Non-Maximum Suppression</strong>. Phần lớn <code>bounding box</code> của cách làm này chỉ có 4 điểm tọa độ nên phương pháp này sẽ bị giới hạn trong việc biểu diễn các văn bản có hình dạng bất thường (ví dụ như bị cong) trong ảnh. Một số mô hình nổi bật cho phương pháp này có thể kể đến như <a href="https://github.com/argman/EAST" target="_blank" rel="noopener noreferrer">EAST</a> hay <a href="https://github.com/MhLiao/TextBoxes" target="_blank" rel="noopener noreferrer">TextBoxes</a>.</li><li>Các phương pháp <strong>Segmentation-based</strong>: xem bài toán <strong>Detection</strong> dưới góc nhìn của một bài toán <strong>Object Segmentation</strong>, nhằm mục đích tìm kiếm các vùng ảnh chứa text ở cấp pixel (trả về xác suất một pixel chứa text). Cách tiếp cận này sẽ phát hiện text bằng cách ước tính vùng giới hạn (<em>bounding area</em>) của text. <code>Bounding box</code> của text sẽ được xây dựng thông qua kết quả phân vùng, nhờ vậy chúng có thể biểu diễn các hình dạng bất thường. Một số đại diện tiêu biểu cho phương pháp này là <a href="https://github.com/MhLiao/DB" target="_blank" rel="noopener noreferrer">DBNet</a>, <a href="https://github.com/whai362/PSENet" target="_blank" rel="noopener noreferrer">PSENet</a>, ...</li><li>Ngoài 2 loại chính trên, còn có một số phương pháp khác như <strong>Character-based methods</strong> (nổi tiếng với <a href="https://github.com/clovaai/CRAFT-pytorch" target="_blank" rel="noopener noreferrer">CRAFT</a>), <strong>Word-based methods</strong>, .... Tuy nhiên, những phương pháp này chỉ là cách phân loại cho một số giải pháp hay model cụ thể nên mình sẽ không đi sâu vào chúng.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-nhận-dạng-văn-bản-text-recognition">3. Nhận dạng văn bản (Text Recognition)<a class="hash-link" href="#3-nhận-dạng-văn-bản-text-recognition" title="Direct link to heading">​</a></h3><p>Bài toán <strong>Text Recognition</strong> có thể phân thành 2 loại sau: Nhận dạng chữ thông thường - <em>Regular Text Recognition</em> và Nhận dạng chữ có sự biến dạng - <em>Irregular Text Recognition</em> (chữ có thể bị nghiêng, cong hoặc bị mờ, méo do nhiều yếu tố). Một số cách làm trước đây là sẽ phát hiện vị trí từng kí tự và nhận dạng chúng bằng <em>Quy hoạch động</em> và thuật toán tìm kiếm được đề cập trong bài báo <a href="https://ieeexplore.ieee.org/abstract/document/6126402/" target="_blank" rel="noopener noreferrer">End-to-end scene text recognition</a>. Các phương pháp này có hạn chế là sẽ không khai thác được mối quan hệ về mặt ngữ nghĩa.</p><p>Giả sử, nếu lựa chọn triển khai từ đầu ở mức ký tự thì ở bài toán <strong>Text Recognition</strong> này, để biết các bounding box thu được có chứa ký tự gì sau khi thực hiện <strong>Text Detection</strong> trên 1 ảnh, ta chỉ cần đơn thuần áp dụng 1 kiến trúc <code>CNN</code>. Cách làm này tương tự như bài toán nhận dạng chữ số viết tay trên bộ dữ liệu <a href="https://www.tensorflow.org/datasets/keras_example" target="_blank" rel="noopener noreferrer">MNIST</a> kinh điển. Và cũng đã có 1 <a href="https://www.kaggle.com/competitions/kuzushiji-recognition" target="_blank" rel="noopener noreferrer">cuộc thi</a> dành cho chữ Kuzushiji (một loại chữ Nhật cổ) với nhiều lời giải hay cũng như các mô hình mới lạ đến từ nhiều thí sinh, đa phần là họ sẽ sử dụng các kiến trúc quen thuộc trong <strong>Object Detection</strong> kết hợp với 1 mạng <code>CNN</code>.</p><p>Tuy nhiên, do những nhược điểm của việc triển khai theo mức ký tự đã được đề cập trong những phần trên, nên trong bài viết này mình chỉ hướng tới những cách tiếp cận mà không chỉ giúp các mô hình <strong>Recognition</strong> có thể nhận dạng được text mà còn học được cả ngữ nghĩa của chúng thay vì chỉ đơn thuần sử dụng 1 mạng <code>CNN</code> để nhận dạng riêng lẻ từng ký tự một.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="a-tiếp-cận-theo-hướng-image-captioning">a) Tiếp cận theo hướng Image Captioning<a class="hash-link" href="#a-tiếp-cận-theo-hướng-image-captioning" title="Direct link to heading">​</a></h4><p>Đơn giản và dễ thấy nhất, ta có thể xem bài toán OCR như một bài toán tạo mô tả bằng ngôn ngữ tự nhiên cho nội dung của hình ảnh (<em>Image Captioning</em>). Trong quá trình hình thành mô tả, ta sẽ liên tục quan sát hình ảnh nhưng đồng thời cũng liên tục tìm cách tạo ra một chuỗi các từ có ý nghĩa. Như vậy, có 2 loại thông tin sẽ cần được xử lý: hình ảnh và mô tả (caption) tương ứng với ảnh đó. Đối với đầu vào là ảnh, ta có thể sẽ sử dụng 1 mạng <code>CNN</code> để trích xuất đặc trưng. Còn đối với đầu vào là caption, <code>RNN</code> sẽ được áp dụng để xử lý dữ liệu chuỗi. Câu hỏi đặt ra ở đây là làm sao có thể kết hợp được thông tin giữa 2 nguồn dữ liệu này, tức nên đưa các mẫu thông tin này vào mô hình như thế nào hay theo thứ tự nào?</p><p>Có 1 <a href="https://github.com/mtanti/where-image2" target="_blank" rel="noopener noreferrer">nghiên cứu</a> đã trình bày 1 sự so sánh có hệ thống cho các cách kết hợp khác nhau hay cụ thể hơn là đánh giá việc xử lý chung hay xử lý riêng biệt từng phần thông tin sẽ hiệu quả hơn.</p><p><img loading="lazy" src="/assets/images/inject-merge-2cc34aeb04c9034ef27de3314e43b021.png" width="755" height="369" class="img_ev3q"></p><p>Ngoài ra, 1 cách giải quyết khác mình tìm hiểu được cho việc tiếp cận theo hướng sinh mô tả cho ảnh là tận dụng sự vượt trội của Cơ chế Attention, được lấy cảm hứng từ 1 <a href="https://github.com/tensorflow/models/tree/master/research/attention_ocr" target="_blank" rel="noopener noreferrer">bài báo của Google</a> và 1 <a href="https://www.tensorflow.org/tutorials/text/image_captioning" target="_blank" rel="noopener noreferrer">bài trong hội nghị ICML</a>. Bằng cách sử dụng mô hình dựa trên Cơ chế Attention (Attention-based) ta có thể biết được phần nào của ảnh sẽ được mô hình tập trung vào khi nó sinh ra caption.</p><center><a href="https://fpt.ai/sites/default/files/2019-11/attention.png" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://fpt.ai/sites/default/files/2019-11/attention.png" width="250" class="img_ev3q"></a></center><h4 class="anchor anchorWithStickyNavbar_LWe7" id="b-mô-hình-crnn-sử-dụng-ctc-loss">b) Mô hình CRNN sử dụng CTC Loss<a class="hash-link" href="#b-mô-hình-crnn-sử-dụng-ctc-loss" title="Direct link to heading">​</a></h4><p>Mạng <a href="https://github.com/bgshih/crnn" target="_blank" rel="noopener noreferrer">CRNN</a> được tạo nên dựa trên những cái nhìn cơ bản nhất: bài toán <strong>Text Recognition</strong> nói 1 cách đơn giản chính là bài toán nhận dạng chuỗi từ ảnh đầu vào, đối việc xử lý dữ liệu ảnh thì mạng phù hợp nhất thường là <code>CNN</code>, còn đối với vấn đề xử lý trình tự thì phù hợp nhất thường là <code>RNN</code>. Từ đó, <a href="https://github.com/bgshih/crnn" target="_blank" rel="noopener noreferrer">CRNN</a> đã ra đời và là sự kết hợp đồng thời giữa 2 mạng <code>CNN</code> và <code>RNN</code> cùng 1 hàm mất mát <a href="https://distill.pub/2017/ctc" target="_blank" rel="noopener noreferrer">CTC</a>. Mô hình này được thiết kế với mục đích giải quyết các nhiệm vụ nhận dạng trình tự dựa trên hình ảnh, chẳng hạn như các bài toán <strong>Scene Text Recognition</strong>. Đây cũng là mô hình phổ biến trong việc nhận dạng chữ in cũng như chữ viết tay và có kết quả rất khả quan dù cho nó có kiến trúc cực kỳ đơn giản.</p><center><a href="https://i0.wp.com/theailearner.com/wp-content/uploads/2021/01/ctc3.jpg" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://i0.wp.com/theailearner.com/wp-content/uploads/2021/01/ctc3.jpg" width="300" class="img_ev3q"></a></center><h4 class="anchor anchorWithStickyNavbar_LWe7" id="c-tiếp-cận-theo-hướng-seq2seq-trong-dịch-máy">c) Tiếp cận theo hướng Seq2Seq trong dịch máy<a class="hash-link" href="#c-tiếp-cận-theo-hướng-seq2seq-trong-dịch-máy" title="Direct link to heading">​</a></h4><p>Kiến trúc <a href="https://github.com/bgshih/crnn" target="_blank" rel="noopener noreferrer">CRNN</a> sử dụng <a href="https://distill.pub/2017/ctc" target="_blank" rel="noopener noreferrer">CTC Loss</a> có 1 hạn chế là ta phải cẩn thận điều chỉnh kiến trúc mô hình để kích thước của vùng nhận thức (receptive field) khớp với số lượng ký tự tối đa có thể dự đoán (max length của text). Một bổ sung phổ biến cho mô hình CRNN có thể được sử dụng để cải thiện dự đoán của văn bản trong ảnh đầu vào là một Cơ chế Attention. Trong hướng tiếp cận này, như thường lệ, đầu tiên ta sẽ sử dụng các mạng <code>CNN</code> để trích xuất đặc trưng ảnh. Sau đó, các đặc trưng này sẽ được chuyển thành chuỗi và truyền qua mạng <code>RNN</code> để có được kết quả cho Cơ chế Attention xử lý.</p><p>Mô hình sử dụng trong quá trình này được lấy cảm hứng và có cách hoạt động tương tự mô hình <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention" target="_blank" rel="noopener noreferrer">Attention Seq2Seq</a> cho bài toán dịch máy. Với 1 bài toán dịch máy từ tiếng Việt sang Anh, ta cần encode 1 chuỗi tiếng Việt thành một vector đặc trưng, còn trong mô hình này, dữ liệu đầu vào sẽ là 1 ảnh.</p><center><a href="https://nanonets.com/blog/content/images/2019/08/attention-ocr.jpg" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://nanonets.com/blog/content/images/2019/08/attention-ocr.jpg" width="300" class="img_ev3q"></a></center><p>Việc huấn luyện mô hình này tương tự như huấn luyện mô hình <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention" target="_blank" rel="noopener noreferrer">Seq2Seq</a>, chúng sử dụng <code>cross-entropy</code> để tối ưu thay vì <a href="https://distill.pub/2017/ctc" target="_blank" rel="noopener noreferrer">CTC Loss</a> như <a href="https://github.com/bgshih/crnn" target="_blank" rel="noopener noreferrer">CRNN</a>. Nghĩa là tại mỗi <code>timestep</code>, mô hình sẽ dự đoán 1 ký tự để tính loss so với nhãn và cập nhật lại trọng số của mô hình. Tuy nhiên đối với mô hình này, các câu ngắn sẽ thường hoạt động tốt hơn, nhưng nếu đầu vào quá dài, mô hình sẽ mất tập trung theo đúng nghĩa đen và ngừng cung cấp các dự đoán hợp lý. Có 2 lý do chính cho việc này:</p><ul><li>Mô hình được huấn luyện sử dụng <code>Teacher Forcing</code> sẽ cung cấp ký tự đúng tại mỗi bước, bất kể kết quả dự đoán là gì. Do đó, mô hình có thể trở nên mạnh mẽ hơn nếu được cung cấp thêm các dự đoán của chính nó.</li><li>Mô hình chỉ có thể biết được đầu ra trước đó của nó thông qua <code>state</code> của <code>RNN</code>. Nếu <code>state</code> này bị hỏng, sẽ không có cách nào để mô hình phục hồi. <a href="https://jalammar.github.io/illustrated-transformer" target="_blank" rel="noopener noreferrer">Transformer</a> đã giải quyết điều này bằng cách sử dụng khả năng tự chú ý (<em>Self-Attention</em>) trong <code>Encoder</code> và <code>Decoder</code>.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="d-các-mô-hình-transformerocr">d) Các mô hình TransformerOCR<a class="hash-link" href="#d-các-mô-hình-transformerocr" title="Direct link to heading">​</a></h4><p>Do kiến trúc <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention" target="_blank" rel="noopener noreferrer">Seq2Seq</a> trên vẫn còn tồn tại 1 số nhược điểm nhất định. Vì vậy, 1 cách phổ biến khác để tăng độ chính xác cho các kiến trúc của bài toán <strong>Recognition</strong> là <a href="https://www.tensorflow.org/text/tutorials/transformer" target="_blank" rel="noopener noreferrer">tận dụng Transformer</a>. Ngoài việc khắc phục được những nhược điểm của mô hình <a href="https://www.tensorflow.org/text/tutorials/nmt_with_attention" target="_blank" rel="noopener noreferrer">Seq2Seq</a>, <a href="https://jalammar.github.io/illustrated-transformer" target="_blank" rel="noopener noreferrer">Transformer</a> cũng đi kèm với rất nhiều ưu điểm khác như có thể xử lý đầu vào có kích thước thay đổi bằng các lớp <code>Self-Attention</code> thay vì các mạng <code>RNN</code> hay <code>CNN</code>. Ngoài ra, nó còn có thể học được các sự <strong>phụ thuộc dài hạn</strong> (<em>long-term dependencies</em>), đây là 1 thử thách trong rất nhiều tác vụ trình tự. Kiến trúc này sẽ có chức năng tương tự như các mạng <code>RNN</code> nhưng điểm khác biệt là nó không yêu cầu xử lý dữ liệu đầu vào theo thứ tự (nghĩa là từ đầu đến cuối). Điều này đồng thời cũng có thể làm giảm đáng kể thời gian cần thiết để huấn luyện một kiến trúc <strong>Recognition</strong> như vậy. Các bạn có thể tham khảo qua 3 link sau để biết thêm 1 số cách có thể tận dụng sức mạnh của Transformer trong các tác vụ OCR:</p><ul><li><a href="https://medium.com/geekculture/scene-text-recognition-using-resnet-and-transformer-c1f2dd0e69ae" target="_blank" rel="noopener noreferrer">Scene Text Recognition Using ResNet and Transformer</a></li><li><a href="https://github.com/microsoft/unilm/tree/master/trocr" target="_blank" rel="noopener noreferrer">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</a></li><li><a href="https://pbcquoc.github.io/vietocr" target="_blank" rel="noopener noreferrer">VietOCR - Nhận Dạng Tiếng Việt Sử Dụng Mô Hình Transformer và AttentionOCR</a></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="e-bổ-sung">e) Bổ sung<a class="hash-link" href="#e-bổ-sung" title="Direct link to heading">​</a></h4><p>Với riêng bài toán <strong>Recognition</strong> này cho tiếng Việt, có tool <a href="https://pbcquoc.github.io/vietocr" target="_blank" rel="noopener noreferrer">VietOCR</a> vừa đề cập ở trên khá nổi tiếng, đã được đóng gói thành thư viện, các bạn có thể sử dụng ngay. Ngoài ra, team <a href="https://github.com/clovaai" target="_blank" rel="noopener noreferrer">Clova AI</a> cũng đã có đề xuất 1 <a href="https://github.com/clovaai/deep-text-recognition-benchmark" target="_blank" rel="noopener noreferrer">pipeline</a> mà phù hợp với hầu hết các mồ hình <strong>STR</strong>. Sử dụng <a href="https://github.com/clovaai/deep-text-recognition-benchmark" target="_blank" rel="noopener noreferrer">pipeline</a> này cho phép các module thể hiện sự đóng góp của mình 1 cách khôn ngoan hơn về độ chính xác, tốc độ và nhu cầu bộ nhớ bằng cách sử dụng nhất quán 1 bộ dữ liệu <code>Train</code> và <code>Validate</code>. Team <a href="https://github.com/clovaai" target="_blank" rel="noopener noreferrer">Clova AI</a> này cũng chính là tác giả của <a href="https://github.com/clovaai/CRAFT-pytorch" target="_blank" rel="noopener noreferrer">CRAFT</a>, một mô hình <strong>Detection</strong> khá nổi tiếng và cũng là tác giả cho phương pháp đánh giá <a href="https://github.com/clovaai/CLEval" target="_blank" rel="noopener noreferrer">CLEval</a> sẽ được trình bày ở phần <a href="#iii-c%C3%A1c-ph%C6%B0%C6%A1ng-ph%C3%A1p-%C4%91%C3%A1nh-gi%C3%A1-evaluation">III</a>.</p><p>Bên cạnh đó, việc phát sinh chuỗi hiện tại của tất cả mô hình đều chỉ đơn giản là sử dụng <strong>Tìm kiếm tham lam</strong> (<em>Greedy search</em>) để tìm các <code>token</code> có xác suất có điều kiện cao nhất tại mỗi <code>timestep</code>. Tuy nhiên, với cách làm này, chuỗi được phát sinh có thể không phải là chuỗi có xác suất cao nhất. Điều này có thể được giải quyết bằng <strong>Tìm kiếm Vét cạn</strong> (<em>Exhaustive Search</em>), tức kiểm tra tất cả các chuỗi đầu ra có thể và trả về chuỗi có xác suất có điều kiện cao nhất, nhưng chi phí tính toán cho giải thuật này là quá lớn. Vì vậy, ta có thể sẽ sử dụng 1 thuật toán cải tiến hơn là <strong>Tìm kiếm Chùm</strong> (<em>Beam search</em>) để cân bằng giữa chi phí tính toán và chất lượng tìm kiếm bằng cách giữ lại <code>N</code> chuỗi có xác suất lớn theo đường đi tốt nhất, cuối cùng chuỗi được chọn sẽ là chuỗi có xác suất cao nhất trong <code>N</code> chuỗi đó. Hay xa hơn nữa là tích hợp <strong>Mô hình ngôn ngữ</strong> (<em>language model</em>) để giải mã đầu ra theo ngữ cảnh của bài toán <strong>Text Recognition</strong>. Các bạn có thể xem qua các project của <a href="https://github.com/githubharald" target="_blank" rel="noopener noreferrer">Harald Scheidl</a>, một người làm khá nhiều về decode, để tìm hiểu thêm cách áp dụng các kỹ thuật này.</p><p>Ngoài ra, nếu các bạn muốn những mô hình phức tạp hay hiện đại hơn nữa thì mình đề xuất các bạn có thể lên <a href="https://paperswithcode.com" target="_blank" rel="noopener noreferrer">Papers With Code</a> của 2 bộ dữ liệu ICDAR <a href="https://paperswithcode.com/dataset/icdar-2013" target="_blank" rel="noopener noreferrer">2013</a> và <a href="https://paperswithcode.com/dataset/icdar-2015" target="_blank" rel="noopener noreferrer">2015</a> để lựa chọn các mô hình cho phù hợp với nhu cầu.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iii-các-phương-pháp-đánh-giá-evaluation">III. Các phương pháp đánh giá (Evaluation)<a class="hash-link" href="#iii-các-phương-pháp-đánh-giá-evaluation" title="Direct link to heading">​</a></h2><p>Bên trên chỉ là một số hướng tiếp cận dễ thấy nhất, được trình bày theo sự hiểu biết của mình nên cũng không tránh khỏi sơ suất. Nếu có chỗ nào chưa được đúng mong mọi người góp ý chứ đừng ném đá nhé. 😅</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-metrics-đánh-giá-text-detection-và-end-to-end">1. Metrics đánh giá Text Detection và End-to-End<a class="hash-link" href="#1-metrics-đánh-giá-text-detection-và-end-to-end" title="Direct link to heading">​</a></h3><p>Để đánh giá kết hợp 2 tác vụ phát hiện và nhận dạng các ký tự trong ảnh, phương pháp thường được sử dụng là <code>IoU</code> - <a href="https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_blank" rel="noopener noreferrer">Intersection over Union</a> kết hợp với Các từ được công nhận chính xác - <code>CRW</code>. Với giai đoạn <strong>Text Detection</strong>, phương pháp <code>IoU</code> chỉ chấp nhận <code>bounding box</code> được dự đoán là đúng khi và chỉ khi giá trị <code>IoU</code> cho 2 <code>bounding box</code> được dự đoán thỏa 1 <code>threshold</code> nhất định (thường &gt; 0.5). Mặc dù <code>IoU</code> được sử dụng rộng rải nhưng cách tính của nó lại không phù hợp đối với các bài toán cần độ chi tiết và chính xác cao, điều này là rất quan trọng đối với các tác vụ <code>OCR</code>.</p><p>Với giai đoạn nhận dạng văn bản trong ảnh, phương phương pháp <code>CRW</code> được sử dụng như một cách đánh giá nhị phân đơn thuần, khi tất cả các văn bản được nhận dạng trùng hoàn toàn với nhãn thì sẽ được tính là 1, ngược lại sẽ là 0. Vì vậy, giới hạn của phương pháp này là không thể đưa ra các chỉ số đánh giá khác nhau cho một kết quả nhận dạng vô lý và một kết quả gần như chính xác.</p><p>Từ trên có thể thấy, phương pháp đánh giá <code>IoU</code> kết hợp với <code>CRW</code> có nhiều mặt hạn chế. Vì vậy ở đây, mình đề xuất 1 phương pháp mới có tên là <a href="https://github.com/clovaai/CLEval" target="_blank" rel="noopener noreferrer">CLEval</a>. Phương pháp này sẽ giúp đánh giá chính xác hơn cho cả 2 giai đoạn đã đề cập. Cụ thể, nó có thể đánh giá chính xác trong trường hợp 2 <code>bounding box</code> khi kết hợp lại chính là nhãn cần phát hiện. Trong khi đó phương pháp <code>IoU</code> lại có thể không chấp nhận một trong 2 hoặc cả 2 box vì ngưỡng được chọn thường &gt; 0.5. Còn ở giai đoạn <strong>Text Recognition</strong>, phương pháp sẽ trả về các chỉ số khác nhau tùy thuộc vào độ tương đồng của dự đoán so với nhãn.</p><p>Ngoài ra, phương pháp này còn có thể đánh giá được cho riêng bài toán <strong>Text Detection</strong>, vì đặc thù của bài toán này là không có các nhãn văn bản trong các <code>bounding box</code> nên tùy vào bài toán mà <a href="https://github.com/clovaai/CLEval" target="_blank" rel="noopener noreferrer">CLEval</a> sẽ có sự khác nhau trong các thành phần tính toán của nó.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-metrics-đánh-giá-với-riêng-text-recognition">2. Metrics đánh giá với riêng Text Recognition<a class="hash-link" href="#2-metrics-đánh-giá-với-riêng-text-recognition" title="Direct link to heading">​</a></h3><p>Với bài toán này thì các bạn có thể sử dụng các phương pháp đánh giá tương tự với các công trình liên quan trước đó cho bài toán <strong>Regconition</strong> theo mức chuỗi. Các phương pháp đánh giá thường bao gồm: <strong>Character Accuracy</strong>, <strong>Sequence Accuracy</strong> và <strong>Character Error Rate</strong> - <code>CER</code></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tham-khảo">Tham khảo<a class="hash-link" href="#tham-khảo" title="Direct link to heading">​</a></h2><ul><li><a href="https://pbcquoc.github.io" target="_blank" rel="noopener noreferrer">https://pbcquoc.github.io</a></li><li><a href="https://labelyourdata.com/articles/ocr-with-deep-learning" target="_blank" rel="noopener noreferrer">https://labelyourdata.com/articles/ocr-with-deep-learning</a></li><li><a href="https://towardsdatascience.com/ocr-101-all-you-need-to-know-e6a5c5d5875b" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/ocr-101-all-you-need-to-know-e6a5c5d5875b</a></li><li><a href="https://towardsdatascience.com/segmentation-in-ocr-10de176cf373" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/segmentation-in-ocr-10de176cf373</a></li><li><a href="https://nanonets.com/blog/attention-ocr-for-text-recogntion" target="_blank" rel="noopener noreferrer">https://nanonets.com/blog/attention-ocr-for-text-recogntion</a></li><li><a href="https://nanonets.com/blog/id-card-digitization-deep-learning" target="_blank" rel="noopener noreferrer">https://nanonets.com/blog/id-card-digitization-deep-learning</a></li><li><a href="https://nanonets.com/blog/deep-learning-ocr" target="_blank" rel="noopener noreferrer">https://nanonets.com/blog/deep-learning-ocr</a></li><li><a href="https://nanonets.com/blog/handwritten-character-recognition" target="_blank" rel="noopener noreferrer">https://nanonets.com/blog/handwritten-character-recognition</a></li><li><a href="https://theailearner.com/optical-character-recognition" target="_blank" rel="noopener noreferrer">https://theailearner.com/optical-character-recognition</a></li><li><a href="https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_blank" rel="noopener noreferrer">https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/OCR/ocr.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Lấy-mẫu-con/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Lấy mẫu con</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/intro"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Giới thiệu</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#i-giới-thiệu" class="table-of-contents__link toc-highlight">I. Giới thiệu</a><ul><li><a href="#1-các-khái-niệm-cơ-bản" class="table-of-contents__link toc-highlight">1. Các khái niệm cơ bản</a></li><li><a href="#2-phân-loại-hình-ảnh-chứa-văn-bản" class="table-of-contents__link toc-highlight">2. Phân loại hình ảnh chứa văn bản</a></li><li><a href="#3-ocr-và-deep-learning" class="table-of-contents__link toc-highlight">3. OCR và Deep Learning</a></li><li><a href="#4-các-bước-triển-khai-chính" class="table-of-contents__link toc-highlight">4. Các bước triển khai chính</a></li><li><a href="#5-một-số-dataset-cho-văn-bản-phi-cấu-trúc" class="table-of-contents__link toc-highlight">5. Một số dataset cho văn bản phi cấu trúc</a></li><li><a href="#6-một-số-công-cụ-mã-nguồn-mở" class="table-of-contents__link toc-highlight">6. Một số công cụ mã nguồn mở</a></li></ul></li><li><a href="#ii-các-phương-pháp-tiếp-cận" class="table-of-contents__link toc-highlight">II. Các phương pháp tiếp cận</a><ul><li><a href="#1-khởi-nguồn-và-lý-do-tiếp-cận-bằng-deep-learning" class="table-of-contents__link toc-highlight">1. Khởi nguồn và lý do tiếp cận bằng Deep Learning</a></li><li><a href="#2-phát-hiện-văn-bản-text-detection" class="table-of-contents__link toc-highlight">2. Phát hiện văn bản (Text Detection)</a></li><li><a href="#3-nhận-dạng-văn-bản-text-recognition" class="table-of-contents__link toc-highlight">3. Nhận dạng văn bản (Text Recognition)</a></li></ul></li><li><a href="#iii-các-phương-pháp-đánh-giá-evaluation" class="table-of-contents__link toc-highlight">III. Các phương pháp đánh giá (Evaluation)</a><ul><li><a href="#1-metrics-đánh-giá-text-detection-và-end-to-end" class="table-of-contents__link toc-highlight">1. Metrics đánh giá Text Detection và End-to-End</a></li><li><a href="#2-metrics-đánh-giá-với-riêng-text-recognition" class="table-of-contents__link toc-highlight">2. Metrics đánh giá với riêng Text Recognition</a></li></ul></li><li><a href="#tham-khảo" class="table-of-contents__link toc-highlight">Tham khảo</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Tài liệu</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Hướng dẫn</a></li></ul></div><div class="col footer__col"><div class="footer__title">Về chúng tôi</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://protonx.ai/san-pham/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Website<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 ProtonX</div></div></div></footer></div>
<script src="/assets/js/runtime~main.9013f7a9.js"></script>
<script src="/assets/js/main.1814ed7f.js"></script>
</body>
</html>